# change iterations to 1001 and compute cost1001
beta <- (grad.descent(x,1001))
cost <- t(mat.or.vec(1,m))
for(i in 1:m) {
cost[i,1] <-  (1 /(2*m)) * (h(x[i,2],beta[1,1],beta[1,2])- y[i,])^2
}
cost1001 <- colSums(cost)
# does this difference meet your convergence criteria?
print(cost1000 - cost1001)
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type="l",
xlab="x",
ylab=expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c (2,2), c (3,8), col="green", lty=2)          # vertical
text (2.1,7, "Closedform solution",col="red",pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x) # update for graph
ftrace <- c(ftrace, f(x)) # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue",pos= 4)
# print final value of x
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type="l",
xlab="x",
ylab=expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)          # vertical
text (2.1,7, "Closedform solution",col="red",pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x) # update for graph
ftrace <- c(ftrace, f(x)) # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue",pos= 4)
# print final value of x
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type="l",
xlab="x",
ylab=expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x) # update for graph
ftrace <- c(ftrace, f(x)) # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue",pos= 4)
# print final value of x
print(x) # x converges to 2.0
lines (xtrace, ftrace, type="b", col="blue")
plot()
plot(xs, f (xs))
lines (xtrace, ftrace, type="b", col="blue")
plot(xs, f (xs), xlim = c(1.5, 2.5))
lines (xtrace, ftrace, type="b", col="blue")
plot(xs, f (xs), xlim = c(1.9, 2.1))
lines (xtrace, ftrace, type="b", col="blue")
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x)      # update for graph
ftrace <- c(ftrace, f(x))   # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue", pos= 4)
# print final value of x
print(dim(xtrace))
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x)      # update for graph
ftrace <- c(ftrace, f(x))   # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue", pos= 4)
# print final value of x
print(length(xtrace))
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x)      # update for graph
ftrace <- c(ftrace, f(x))   # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue", pos= 4)
# print final value of x
print(length(xtrace))
print(length(ftrace))
print(x) # x converges to 2.0
f <-function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4,len=20)
plot (xs , f (xs ), type=”l”,xlab=”x”,ylab=expression(1.2(x−2)ˆ2 +3.2))
xs <− seq(0,4,len=20)
plot (xs , f(xs), type=”l”,xlab=”x”,ylab=expression(1.2(x−2)^2 +3.2))
xs <− seq(0,4,len=20)
plot (xs , f(xs),
type = ”l”,
f <- function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4,len=20)
plot(xs, f(xs),
type = ”l”,
xs <− seq(0,4,len=20)
plot(xs, f(xs),
type = ”l”,
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = ”l”,
plot(xs, f(xs))
plot(xs, f(xs), type = "l")
plot(xs, f(xs), type = "l", xlab = "x")
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col =”red”,lty=2)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red"", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}lines ( xtrace , ftrace , type=”b”,col=”blue”)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines ( xtrace , ftrace , type="b", col="blue")
text (0.5,6, "Gradient Descent",col=”blue”,pos=4)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines ( xtrace , ftrace , type="b", col="blue")
text (0.5,6, "Gradient Descent", col = "blue", pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f, grad , stepsize =0.6, nIterations =100, xtracep=TRUE, ftracep=TRUE)
plot (xs, f(xs ), type="l",xlab="x", ylab=expression(1.2(x−2)^2 +3.2))
lines ( result $xtrace , result $ ftrace , type="b",col="blue")
text (0.5,6, "Gradient Descent with steepest ()", col ="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f,
grad,
stepsize = 0.6,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot (xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace , type ="b", col ="blue")
text (0.5,6, "Gradient Descent with steepest ()", col ="blue",pos=4)
plot(xs, f(xs ), type= "l", xlab="x", ylab=expression(1.2(x−2)^2 +3.2))
x <− 0.1
xtrace <− x
ftrace <− f(x)
for (step in 1:100) {
x <− x − grad(x)/secondGrad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines(xtrace, ftrace , type="b",col="blue")
text (0.5,6, "Newton's Gradient Descent",col="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad ,
nIterations =100,
xtracep=TRUE,
ftracep=TRUE)
plot (xs , f (xs ), type="l",xlab="x",ylab=expression(1.2(x−2)^2 +3.2))
lines (result$xtrace, result$ftrace , type ="b", col="blue")
text (0.5,6, "Gradient Descent with scg ()", col ="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad ,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f (xs), type = "l", xlab = "x",ylab=expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace, type = "b", col = "blue")
text(0.5, 6, "Gradient Descent with scg ()", col = "blue", pos = 4)
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
x <− 0.1
xtrace <− x
ftrace <− f(x)
for (step in 1:100) {
x <− x − grad(x) / secondGrad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines(xtrace, ftrace, type = "b", col = "blue")
text (0.5, 6, "Newton's Gradient Descent", col = "blue", pos = 4)
f <- function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4, len = 20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c(2,2), c(3,8), col = "red", lty = 2)
text (2.1, 7, "Closed−form solution", col = "red", pos = 4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace, x)
ftrace <− c(ftrace, f(x))
}
lines( xtrace , ftrace , type="b", col = "blue")
text(0.5, 6, "Gradient Descent", col = "blue", pos = 4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f,
grad,
stepsize = 0.6,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace , type = "b", col = "blue")
text (0.5, 6, "Gradient Descent with steepest ()", col = "blue", pos = 4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f (xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace, type = "b", col = "blue")
text(0.5, 6, "Gradient Descent with scg ()", col = "blue", pos = 4)
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
f <- function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4, len = 20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c(2,2), c(3,8), col = "red", lty = 2)
text (2.1, 7, "Closed−form solution", col = "red", pos = 4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace, x)
ftrace <− c(ftrace, f(x))
}
lines( xtrace , ftrace , type="b", col = "blue")
text(0.5, 6, "Gradient Descent", col = "blue", pos = 4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f,
grad,
stepsize = 0.6,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace , type = "b", col = "blue")
text(0.5, 6, "Gradient Descent with steepest ()", col = "blue", pos = 4)
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
x <− 0.1
xtrace <− x
ftrace <− f(x)
for (step in 1:100) {
x <− x − grad(x) / secondGrad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines(xtrace, ftrace, type = "b", col = "blue")
text(0.5, 6, "Newton's Gradient Descent", col = "blue", pos = 4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f (xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace, type = "b", col = "blue")
text(0.5, 6, "Gradient Descent with scg ()", col = "blue", pos = 4)
install.packages("RcppOctave")
install.packages("RcppOctave")
install.packages("RcppOctave", source)
install.packages("RcppOctave", type="source")
