<<<<<<< HEAD
=======
<<<<<<< HEAD
Y = log(defsloc)
# plot the transformed data
plot(X, Y,
xlab = "Log PrepTime/SLOC",
ylab = "Log Defects/SLOC"
)
n <- 100
mu <- c(-2, 2)
sigma <- matrix(c(1, 0.5, 0.5, 1), nrow = 2)
n
mu
sigma
library(MASS)
set.seed(123)
head(X)
X <- mvrnorm(n, mu, sigma)
#plot(X[, 1], X[, 2])
source("../R/matlab.R")
# read the matrix from Matlab table
x_table <- read.csv('table.csv', header = FALSE)
# convert to matrix
X <- as.matrix(x_table)
xbar <- colMeans(X)   # calculate the means of the columns
# get the eigenvectors and eigenvalues of the covariance matrix
eig <- mlab_eig(cov(X))  # calculate the eigenVv of the covariance of X
D <- eig$D
V <- eig$V
# center the data
Xc <- X - matrix(1, n, 1) %*% xbar
# Sphere the data
z <- D %^% (-1/2) %*% t(V) %*% t(Xc)  # using a custom built ^ operator
Z <- t(z)                             # transpose to plot
plot(Z[, 1], Z[, 2])
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
# enter data as rows
X = matrix(c(4, 3, -4, 5), nrow=2, byrow = TRUE)
X
# get theta
theta <- pi /3
# obtain the projection matrix
c2 <- cos(theta)^2
cs <- cos(theta)*sin(theta)
s2 <- sin(theta)^2
P <- array(c(c2, cs, cs, s2), dim = c(2,2))
cat("Projection matrix at angle:\n")
P
# obtain the projection matrix
c2 <- cos(theta)^2
cs <- cos(theta)*sin(theta)
s2 <- sin(theta)^2
P <- array(c(c2, cs, cs, s2), dim = c(2,2))
cat("Projection matrix at angle:\n")
P
source("../R/matlab.R")
Xp <- X %*% P
Xp
Px <- matrix(c(1, 0), nrow=2)
Xpx <- X %*% Px
cat("Ortogonal projection:\n")
Px
cat("Projection over the x-axis\n")
Xpx
plot(X[, 1], X[, 2],
xlim = c(-8, 8),
ylim = c(-8, 8),
pch = 20,
cex = 1.55
)
projectionMatrixLine(P, col = 'green')   # draw the projection line at angle
abline(h=0, v=0)             # lines at (0, 0)
points(Xpx[1], 0, pch=11)    # plot point projections on x-axis
points(Xpx[2], 0, pch=11)    # plot point projections on x-axis
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(R.matlab)
# read the Matlab array
yeast.mat <- readMat("../data/yeast.mat", fixNames = TRUE)
names(yeast.mat)
data <- yeast.mat$data
n <- nrow(data)
p <- ncol(data)
n;p
source("../R/matlab.R")
r <- m.repmat(colSums(data)/n, n, 1)
m.size(r)
A <- matrix(c(1,2,3,4), ncol = 2, byrow = F)
A
r <- m.repmat(A, 2, 3)
r
# column sums
cs <- colSums(data) / n        # numeric vector, no matrix
# convert to matrix 1x17, like in Matlab
mcs <- matrix(cs, nrow = 1)
dim(mcs)
source("../R/matlab.R")
# create a new matrix 384x17
r <- m.repmat(mcs, n, 1)
dim(r)
source("../R/matlab.R")
# column sums
cs <- colSums(data) / n        # numeric vector, no matrix
# convert to matrix 1x17, like in Matlab
mcs <- matrix(cs, nrow = 1)
datac <- data - m.repmat(mcs, n, 1)  # center the data
dim(datac)
source("../R/matlab.R")
# find the covariance matrix_type
covm <- cov(datac)
eig <- m.eig(covm)
eigvec <- eig$vectors
eigval <- eig$values
eigvald <- matrix(diag(eigval))  # Extract the diagonal elements
# Order in descending order
eigvale <- m.flipud(eigvald)
eigvece <- eigvec[, p:1]
eigvald
eigvale
# Build scree plot
plot(1:length(eigvale), eigvale,
=======
print(dim(xtrace))
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x)      # update for graph
ftrace <- c(ftrace, f(x))   # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue", pos= 4)
# print final value of x
print(length(xtrace))
print(x) # x converges to 2.0
xs <- seq(0, 4,len=20) # create some values
# define the function we want to optimize
f <-  function(x) {
1.2 * (x-2)^2 + 3.2
}
# plot the function
plot(xs, f (xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x-2)^2 +3.2))
# calculate the gradient df/dx
grad <- function(x){
1.2*2*(x-2)
}
# df/dx = 2.4(x-2), if x = 2 then 2.4(2-2) = 0
# The actual solution we will approximate with gradeint descent
# is  x = 2 as depicted in the plot below
lines (c(2,2), c(3,8), col="green", lty=2)
text (2.1,7, "Closed-form solution", col="red", pos=4)
# gradient descent implementation
x <- 0.1       # initialize the first guess for x-value
xtrace <- x    # store x -values for graphing purposes (initial)
ftrace <- f(x) # store y-values (function eval at x) for graphing purposes (initial)
stepFactor <- 0.6 # learning rate 'alpha'
for (step in 1:100) {
x <- x - stepFactor * grad(x) # gradient descent update
xtrace <- c(xtrace, x)      # update for graph
ftrace <- c(ftrace, f(x))   # update for graph
}
lines (xtrace, ftrace, type="b", col="blue")
text (0.5, 6, "Gradient Descent", col="blue", pos= 4)
# print final value of x
print(length(xtrace))
print(length(ftrace))
print(x) # x converges to 2.0
f <-function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4,len=20)
plot (xs , f (xs ), type=”l”,xlab=”x”,ylab=expression(1.2(x−2)ˆ2 +3.2))
xs <− seq(0,4,len=20)
plot (xs , f(xs), type=”l”,xlab=”x”,ylab=expression(1.2(x−2)^2 +3.2))
xs <− seq(0,4,len=20)
plot (xs , f(xs),
type = ”l”,
f <- function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4,len=20)
plot(xs, f(xs),
type = ”l”,
xs <− seq(0,4,len=20)
plot(xs, f(xs),
type = ”l”,
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = ”l”,
plot(xs, f(xs))
plot(xs, f(xs), type = "l")
plot(xs, f(xs), type = "l", xlab = "x")
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col =”red”,lty=2)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red"", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
>>>>>>> bd190047f22cc281a4f837ea6d618c8e55d3b905
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}lines ( xtrace , ftrace , type=”b”,col=”blue”)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines ( xtrace , ftrace , type="b", col="blue")
text (0.5,6, "Gradient Descent",col=”blue”,pos=4)
xs <− seq(0,4, len=20)
plot(xs, f(xs),
type = "l",
xlab = "x",
ylab = expression(1.2(x−2)^2 +3.2))
### df/dx = 2.4(x−2)
### df/dx = 0 −−−> 0 = 2.4x − 4.8 −−−> x = 2
lines (c (2,2), c (3,8), col = "red", lty =2)
text (2.1,7, "Closed−form solution", col="red", pos=4)
### gradient descent
x <− 0.1
xtrace <− x
ftrace <− f(x)
stepFactor <− 0.6 ### try larger and smaller values (0.8 and 0.01)
for (step in 1:100) {
x <− x − stepFactor * grad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines ( xtrace , ftrace , type="b", col="blue")
text (0.5,6, "Gradient Descent", col = "blue", pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f, grad , stepsize =0.6, nIterations =100, xtracep=TRUE, ftracep=TRUE)
plot (xs, f(xs ), type="l",xlab="x", ylab=expression(1.2(x−2)^2 +3.2))
lines ( result $xtrace , result $ ftrace , type="b",col="blue")
text (0.5,6, "Gradient Descent with steepest ()", col ="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− steepest(x, f,
grad,
stepsize = 0.6,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot (xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace , type ="b", col ="blue")
text (0.5,6, "Gradient Descent with steepest ()", col ="blue",pos=4)
plot(xs, f(xs ), type= "l", xlab="x", ylab=expression(1.2(x−2)^2 +3.2))
x <− 0.1
xtrace <− x
ftrace <− f(x)
for (step in 1:100) {
x <− x − grad(x)/secondGrad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines(xtrace, ftrace , type="b",col="blue")
text (0.5,6, "Newton's Gradient Descent",col="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad ,
nIterations =100,
xtracep=TRUE,
ftracep=TRUE)
plot (xs , f (xs ), type="l",xlab="x",ylab=expression(1.2(x−2)^2 +3.2))
lines (result$xtrace, result$ftrace , type ="b", col="blue")
text (0.5,6, "Gradient Descent with scg ()", col ="blue",pos=4)
source("./R/gradientDescents.R")
x <− 0.1
result <− scg(x, f,
grad ,
nIterations = 100,
xtracep = TRUE,
ftracep = TRUE)
plot(xs, f (xs), type = "l", xlab = "x",ylab=expression(1.2(x−2)^2 +3.2))
lines(result$xtrace, result$ftrace, type = "b", col = "blue")
text(0.5, 6, "Gradient Descent with scg ()", col = "blue", pos = 4)
plot(xs, f(xs), type = "l", xlab = "x", ylab = expression(1.2(x−2)^2 +3.2))
x <− 0.1
xtrace <− x
ftrace <− f(x)
for (step in 1:100) {
x <− x − grad(x) / secondGrad(x)
xtrace <− c(xtrace,x)
ftrace <− c(ftrace,f(x))
}
lines(xtrace, ftrace, type = "b", col = "blue")
text (0.5, 6, "Newton's Gradient Descent", col = "blue", pos = 4)
f <- function(x) {
1.2 * (x−2)^2 + 3.2
}
grad <− function(x) {
1.2 * 2 * (x−2)
}
secondGrad <− function(x) {
2.4
}
xs <− seq(0,4, len = 20)
plot(xs, f(xs),
>>>>>>> develop
type = "l",
main = "Scree Plot",
xlab = "Eigenvalue Index",
ylab = "Eigenvalue")
points(1:length(eigvale), eigvale)
pervar = 100*apply(eigvale, 2, cumsum)/sum(eigvale)
# First get the expected sizes of the eigenvalues.
g <- matrix(0, nrow= 1, ncol = p)
for (k in 1:p) {
for (i in k:p) {
g[k] <- g[k] + 1 /i
}
}
g <- g / p
propvar <- eigvale / sum(eigvale)
g[1:4]
propvar[1:4]
# Now for the size of the variance.
avgeig = mean(eigvale);
# Find the length of ind:
ind <- which(eigvale > avgeig)
length(ind)
# So, using 3, we will reduce the dimensionality.
P = eigvece[,1:3]
Xp = datac %*% P
library(lattice)
cloud(Xp[,1]~Xp[,2] * Xp[,3])
library("plot3D")
scatter3D(Xp[,1], Xp[,2], Xp[,3],
col = "blue",
pch = 19,
cex = 0.5,
bty = "b2",
xlab = "PC1",
ylab = "PC2",
zlab = "PC3",
theta = 30,       # 3D viewing direction: azimuth
phi = 15,           # co-latitude
ticktype = "detailed"
)
install.packages("plot3D")
library("plot3D")
scatter3D(Xp[,1], Xp[,2], Xp[,3],
col = "blue",
pch = 19,
cex = 0.5,
bty = "b2",
xlab = "PC1",
ylab = "PC2",
zlab = "PC3",
theta = 30,       # 3D viewing direction: azimuth
phi = 15,           # co-latitude
ticktype = "detailed"
)
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(R.matlab)
source("../R/matlab.R")
# read the Matlab array
lsiex.mat <- readMat("../data/lsiex.mat", fixNames = TRUE)
names(lsiex.mat)
termdoc <- lsiex.mat$termdoc
X <- lsiex.mat$X
# Loads up variables: X, termdoc, docs and words.
# Convert the matrix to one that has columns
# with a magnitude of 1.
n <- m.size(termdoc)[1]
p <- m.size(termdoc)[2]
no <- m.zeros(1, p)
for (i in 1:p) {
no[,i] = norm(matrix(X[, i]), 'f')   # Frobenius norm
termdoc[, i] <- X[, i] / no[,i]
}
no
X
termdoc
q1 <- t(matrix(c(1, 0, 1, 0, 0, 0), nrow=1))
dim(q1)
# Find the cosine of the angle between
# columns of termdoc and query.
# Note that the magnitude of q1 is not 1.
m1 <- norm(q1, 'f')
cosq1a <- t(q1) %*% termdoc / m1
# Note that the magnitude of q2 is 1.
cosq2a = t(q2) %*% termdoc
q2 <- t(matrix(c(1, 0, 0, 0, 0, 0), nrow=1))
dim(q2)
# Find the cosine of the angle between
# columns of termdoc and query.
# Note that the magnitude of q1 is not 1.
m1 <- norm(q1, 'f')
cosq1a <- t(q1) %*% termdoc / m1
# Note that the magnitude of q2 is 1.
cosq2a = t(q2) %*% termdoc
cosq1a
cosq2a
svd(termdoc)
source("../R/matlab.R")
svd_all <- m.svd(termdoc)
u <- svd_all$u
d <- svd_all$d
v <- svd_all$v
# Project the query vectors.
q1t <- t(u[, 1:3]) %*% q1
q2t <- t(u[, 1:3]) %*% q2
# Now find the cosine of the angle between the query
# vector and the columns of the reduced rank matrix,
# scaled by D.
cosq1b = matrix()
cosq2b = matrix()
for (i in 1:5) {
tv <- matrix(t(v[i, 1:3]), ncol=1)
sj <- d[1:3, 1:3] %*% tv
m3 <- norm(sj, 'f')
cosq1b[i] <- t(sj) %*% q1t / (m3 * m1)
cosq2b[i] <- t(sj) %*% q2t / (m3)
}
cosq1b
cosq2b
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(R.matlab)
library(Hmisc)
library(R.matlab)
source("../R/matlab.R")
# read the Matlab array
stockreturns.mat <- readMat("../data/stockreturns.mat", fixNames = TRUE)
names(stockreturns.mat)
stocks <- stockreturns.mat$stocks
head(stocks)
title = "Varimax rotation"
fac.anal <- factanal(stocks, factors = 3, rotation = "varimax")
Lam <- fac.anal$loadings
plot(Lam[,1], Lam[, 2], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 2], labels = 1:10, cex = 0.7, pos = 3)
plot(Lam[,1], Lam[, 3], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 3], labels = 1:10, cex = 0.7, pos=3)
title = "No rotation"
lam.psi <- factanal(stocks, factors = 3)
Lam <- fac.anal$loadings
plot(Lam[,1], Lam[, 2], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 2], labels = 1:10, cex = 0.7, pos = 3)
plot(Lam[,1], Lam[, 3], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 3], labels = 1:10, cex = 0.7, pos=3)
title = "Promax rotation"
lam.psi <- factanal(stocks, factors = 3, rotation = "promax")
Lam <- fac.anal$loadings
plot(Lam[,1], Lam[, 2], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 2], labels = 1:10, cex = 0.7, pos = 3)
plot(Lam[,1], Lam[, 3], xlim = c(-1,1), ylim=c(-1,1), main = title)
abline(h=0, v=0)
text(Lam[, 1], Lam[, 3], labels = 1:10, cex = 0.7, pos=3)
library("plot3D")
n <- 500
theta <- runif(n =  n, min = 0, max = 4*pi)
# Use in the equations for a helix￿
x <- cos(theta)
y <- sin(theta)
z <- 0.1 * (theta)
# Put into a data matrix
# X = [x(,),y(,),z(,)]
X <- matrix(c(x, y, z), 500, 3)
scatter3D(x, y, z,
col = "blue",
pch = 19,
cex = 0.5,
bty = "b2",
xlab = "x",
ylab = "y",
zlab = "z",
theta = -45,       # 3D viewing direction: azimuth
phi = 30,           # co-latitude
ticktype = "detailed"
)
# Set knitr options for knitting code into the report:
# - Don't print out code (echo)
# - Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - Don't clutter R output with messages or warnings (message, warning)
# This _will_ leave error messages showing up in the knitted report
knitr::opts_chunk$set(echo = TRUE,
cache = TRUE,
autodep = TRUE,
cache.comments = FALSE,
message = FALSE,
warning = FALSE,
error = TRUE,
comment = NA)
library(ggplot2)
f <- function (beta) beta**4 - 3 * beta**3 + 2     # build the function
beta <- seq(-2, 4, length.out = 100)               # points for plotting
qplot(beta,
f(beta),
xlab = expression(beta),
ylab = expression(f(beta)),
geom = 'line') +
geom_vline(xintercept = 9/4, col = 'red')
# Set the initial guess and other parameters
beta_old <- 0; beta_new <- 4; gamma <- 0.001; precision <- 0.00001
# Derivative of the function f
f_prime <- function (beta) 4 * beta**3 - 9 * beta**2
# create dataframebet
dframe <- data.frame(beta = rep(beta, 2),
curves = c(f(beta), f_prime(beta)),
type = rep(c("f", "f'"), each = 100))
# plot
p <- ggplot(data = dframe, aes(x = beta, y = curves, colour = type)) +
geom_line() +
geom_segment(aes(x = beta_new,
y = f(beta_new),
xend = beta_new,
yend = f_prime(beta_new),
colour = 'orange'), show.legend = TRUE) +
xlab(expression(beta)) + ylab('y') +
theme(legend.position = c(.2, .85),
legend.background = element_rect(fill = 'gray')) +
scale_colour_manual(name = 'Curves and Segments',
labels = c(
expression(f(beta)),
expression(paste(frac(df(beta), paste('d', beta)), sep = "")),
expression(bar(paste(f(beta[r]), paste(f, "'", (beta[r]), sep = ""))))
),
values = c('#FF033E', '#318CE7', '#FFBF00'),
guide = guide_legend(direction = "horizontal",
title.position = "top",
label.position = "right",
label.theme = element_text(angle = 0, size = 10))
)
# Perform Gradient Descent
i <- 0 # Counter for number of iterations
p <- p + geom_segment(x = beta_new,
y = f(beta_new),
xend = beta_new,
yend = f_prime(beta_new),
colour = '#FFA812')
while (abs(beta_new - beta_old) > precision) {
beta_old <- beta_new
beta_new <- beta_old - gamma * f_prime(beta_old)
p <- p + geom_segment(x = beta_new,
y = f(beta_new),
xend = beta_new,
yend = f_prime(beta_new),
colour = '#FFA812')
i <- i + 1
}
# Print results
p; i; beta_new
## Gibbs sampler in R and Octave
##
## Compare with RcppGibbs/ example in Rcpp package
## Simple Gibbs Sampler Example
## Adapted from Darren Wilkinson's post at
## http://darrenjw.wordpress.com/2010/04/28/mcmc-programming-in-r-python-java-and-c/
##
## Sanjog Misra and Dirk Eddelbuettel, June-July 2011
suppressMessages(library(compiler))
suppressMessages(library(rbenchmark))
install.packages("rbenchmark")
## Gibbs sampler in R and Octave
##
## Compare with RcppGibbs/ example in Rcpp package
## Simple Gibbs Sampler Example
## Adapted from Darren Wilkinson's post at
## http://darrenjw.wordpress.com/2010/04/28/mcmc-programming-in-r-python-java-and-c/
##
## Sanjog Misra and Dirk Eddelbuettel, June-July 2011
suppressMessages(library(compiler))
suppressMessages(library(rbenchmark))
suppressMessages(library(RcppOctave))
install.packages("RcppOctave")
<<<<<<< HEAD
## Gibbs sampler in R and Octave
##
## Compare with RcppGibbs/ example in Rcpp package
## Simple Gibbs Sampler Example
## Adapted from Darren Wilkinson's post at
## http://darrenjw.wordpress.com/2010/04/28/mcmc-programming-in-r-python-java-and-c/
##
## Sanjog Misra and Dirk Eddelbuettel, June-July 2011
suppressMessages(library(compiler))
suppressMessages(library(rbenchmark))
suppressMessages(library(RcppOctave))
## This is Darren Wilkinsons R code (with the corrected variance)
## But we are returning only his columns 2 and 3 as the 1:N sequence
## is never used below
Rgibbs <- function(N,thin) {
mat <- matrix(0,ncol=2,nrow=N)
x <- 0
y <- 0
for (i in 1:N) {
for (j in 1:thin) {
x <- rgamma(1,3,y*y+4)
y <- rnorm(1,1/(x+1),1/sqrt(2*(x+1)))
}
mat[i,] <- c(x,y)
}
mat
}
## We can also try the R compiler on this R function
RCgibbs <- cmpfun(Rgibbs)
## Octave function
Mgibbs <- OctaveFunction('
function mat = Mgibbs(N, thin)
mat = zeros(N, 2);
x = 0;
y = 0;
for i = 1:N
for j = 1:thin
x = randg(3) / (y*y+4);
y = randn(1)*1/sqrt(2*(x+1)) + 1/(x+1);
end
mat(i,:) = [ x, y ];
end
end
')
## Octave docs:
##     `gamma (a, b)' for `a > -1', `b > 0'
##               r = b * randg (a)
## CORRECTION:  set.seed(42); .O$gam(2,3)
##         ==   set.seed(42); rgamma(1,2,1,3)
##         ==   set.seed(42); rgamma(1,2,1)*3
##         ==   set.seed(42); o_rgamma(1,1,2,3)
# set.seed(42); .O$gam(47,88); set.seed(42); rgamma(1,47,1,88);  o_rgamma(1,1,47,88)
# set.seed(42); .O$gam(47,88); set.seed(42); rgamma(1,47,1,88); set.seed(42); o_rgamma(1,1,47,88)
set.seed(42)
matR <- Rgibbs(1000,10)
set.seed(42)
matO <- Mgibbs(1000,10)
stopifnot(all.equal(matR, matO))
##print(summary(matR))
##print(summary(matO))
## also use rbenchmark package
N <- 1000
thn <- 100
res <- benchmark(Rgibbs(N, thn),
RCgibbs(N, thn),
Mgibbs(N, thn),
columns=c("test", "replications", "elapsed",
"relative", "user.self", "sys.self"),
order="relative",
replications=10)
print(res)
=======
install.packages("RcppOctave", source)
install.packages("RcppOctave", type="source")
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
install.packages(c("digest", "dygraphs", "fields", "ggplot2", "git2r", "Hmisc", "htmlTable", "jsonlite", "nycflights13", "openssl", "pracma", "psych", "RcppArmadillo", "rmarkdown", "rsconnect", "RSQLite", "selectr", "shiny", "sp", "tidyr", "xml2", "zoo"))
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
fx <- cxxfunction(signature(x = "numeric"), 'NumericVector xx(x);
return wrap(std::accumulate(xx.begin(), xx.end(), 0.0));'
, plugin = "Rcpp"
fx <- cxxfunction(signature(x = "numeric"), 'NumericVector xx(x);return wrap(std::accumulate(xx.begin(), xx.end(), 0.0));', plugin = "Rcpp"
library(Rcpp)
library(inline)
fx <- cxxfunction(signature(x = "numeric"), 'NumericVector xx(x);return wrap(std::accumulate(xx.begin(), xx.end(), 0.0));', plugin = "Rcpp")
fx <- cxxfunction(signature(x = "numeric"), 'NumericVector xx(x); return wrap(std::accumulate(xx.begin(), xx.end(), 0.0));', plugin = "Rcpp")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
sessionInfo()
View(y)
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
x <- leukemia[, 1:38]   #: take only the first 38 columns
n <- nrow(x)      # number of rows
p <- ncol(x)      # number of columns
y <- matrix(0, n, p)
for (i in 1:n) {
sig <- sd(x[i, ])
mu <- mean(x[i, ])
y[i, ] <- (x[i, ] - mu) / sig
}
library(R.matlab)
>>>>>>> develop
# Set knitr options for knitting code into the report:
# - Don't print out code (echo)
# - Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - Don't clutter R output with messages or warnings (message, warning)
# This _will_ leave error messages showing up in the knitted report
knitr::opts_chunk$set(echo = TRUE,
cache = TRUE,
autodep = TRUE,
cache.comments = FALSE,
message = FALSE,
warning = FALSE,
error = TRUE,
comment = NA)
library(RcppOctave)
.CallOctave("version")
<<<<<<< HEAD
fX <- OctaveFunction("
function [X] = mvnrandom()
pkg load statistics
% First generate some 2-D multivariate normal
% random variables, with mean MU and
% covariance SIGMA. This uses a Statistics
% Toolbox function.
n = 100;
mu = [-2, 2];
sigma = [1,.5;.5,1];
X = mvnrnd(mu,sigma,n);
% plot(X(:,1),X(:,2),'.')
end
")
X <- fX()
dim(X)
sphered <- OctaveFunction("function [Z] = sphered(X, n=100)
% Now sphere the data.
xbar = mean(X);
% Get the eigenvectors and eigenvalues of the
% covariance matrix.
[V,D] = eig(cov(X));
D
% Center the data.
Xc = X - ones(n,1)*xbar;
% Sphere the data.
Z = ((D)^(-1/2)*V'*Xc')';
% plot(Z(:,1),Z(:,2),'.')
end
")
Z <- sphered(X)
plot(Z[,1], Z[,2])
=======
.CallOctave("sqrt", 10)
.CallOctave("eye", 3)
.CallOctave("eye", 3, 2)
install.packages("RcppOctave")
<<<<<<< HEAD
library(Rcpp)
sourceCpp("../../src/MatrixExample.cpp")
M <- matrix((1:16)^2, 4)
class(M)
ME <- MatrixExample(M)
class(ME)
ME
library(Rcpp)
myroot <- cppFunction('double myroot(double x) { return ::sqrt(x); }')
myroot(16)
library(Rcpp)               ## recent version for sourceCpp()
sourceCpp("../../src/eigenEx.cpp")    ## converts source file into getEigen() we can call
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(Rcpp)
install.packages("Rcpp")
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(Rcpp)
library(Rcpp)
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(Rcpp)
fibR <- function(n){
## I commented this out because I am not handling errors in the C++ code,
## so the code is comparable
# if (n < 0) {
#  stop("Argument n must be non-negative integer.\n")
# } else
if (n == 0) {
return(0)
} else if (n == 1) {
return(1)
} else {
Recall(n-1) + Recall(n-2)
}
}
n <- 10:26
time.to.calculate <- sapply(n, function(x) system.time(fibR(x))[3])
plot(n, time.to.calculate, xlab="n", ylab="seconds",
main="Fibonacci F calculation time", type="b")
knitr::opts_chunk$set(echo = TRUE, comment = NA, error = TRUE)
library(Rcpp)
library(Rcpp)
library(inline)
install.packages("inline")
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
fun <- cxxfunction(signature(A = "numeric"), body = src, plugin="Rcpp")
fun(matrix(1,4,4))
library(Rcpp)
myroot <- cppFunction('double myroot(double x) { return ::sqrt(x); }')
myroot(16)
library(Rcpp)
library(inline)
src <- '
Rcpp::NumericMatrix Am(A);
int nrows = Am.nrow();
for (int j = 1; j < nrows; j++) {
Am(j,_) = Am(j,_) + Am(j-1,_);
}
return Am;
'
library(Rcpp)
fibR <- function(n){
## I commented this out because I am not handling errors in the C++ code,
## so the code is comparable
# if (n < 0) {
#  stop("Argument n must be non-negative integer.\n")
# } else
if (n == 0) {
return(0)
} else if (n == 1) {
return(1)
} else {
Recall(n-1) + Recall(n-2)
}
}
n <- 10:26
time.to.calculate <- sapply(n, function(x) system.time(fibR(x))[3])
plot(n, time.to.calculate, xlab="n", ylab="seconds",
main="Fibonacci F calculation time", type="b")
PKG_CXXFLAGS=`Rscript -e 'Rcpp:::CxxFlags()'` \
PKG_LIBS=`Rscript -e 'Rcpp:::LdFlags()'`  \
R CMD SHLIB fibWrap.cpp
library(Rcpp)
dyn.load("fibWrap.so")
.Call("fibWrapper", 5) # Evaluates fibWrapper() at 5
.Call("fibWrapper", 9)
.Call("fibWrapper", 10)
library(Rcpp)
sourceCpp("../../src/fibBest.cpp")
n2 <- 10:(26+12) # Some extra 12 values
time.to.calculateCpp <- sapply(n2, function(x) system.time(fibCpp(x))[3])
plot(n2, c(time.to.calculate, rep(NA,12)), xlab="n", ylab="seconds",
main="Fibonacci F calculation time", type="b")
lines(n2, time.to.calculateCpp, type="b", col="Red")
legend("topleft", inset=0.01, legend=c("R","C++"), pch=1, col=1:2)
#include <Rcpp.h>
#include <cmath>        // std::exp(double)
#include <valarray>     // std::valarray, std::exp(valarray)
using namespace Rcpp;
// [[Rcpp::export]]
NumericVector dnormCpp(NumericVector x, double mu, double sigma) {
// const double MYPI = 3.141592;
// I think PI is available in cmath
int n = x.size();
NumericVector d(n);
NumericVector ret(n);
for(int i = 0; i < n; i++){
d[i] = (x[i] - mu)/sigma;
d[i] *= d[i];
ret[i] = exp(-0.5*d[i])/(sqrt(2*PI)*sigma);
}
return(ret);
}
require(RcppEigen)
sourceCpp("../../src/xtSx.cpp")
S <- matrix(c(1,.5,.5,1),ncol=2)
Y <- c(3,2)
X <- matrix(c(1,0,1,1), ncol=2)
solve(t(X)%*%solve(S)%*%X)%*%t(X)%*%solve(S)%*%Y
xtSx(X,S,Y)
sourceCpp("../../src/xtSxMod.cpp")
timeCppFast <- numeric(length(n))
k <- 0
set.seed(1)
for(i in n){
k <- k + 1
Y <- rnorm(i)
X <- matrix(rnorm(i*p), ncol=p)
S <- matrix(0, i, i)
for(j in 1:i){
S[j,] = 0.5^abs(j - 1:i)
}
timeCppFast[k] <- system.time(xtSxMod(X,S,Y))[3]
}
=======
>>>>>>> develop
>>>>>>> bd190047f22cc281a4f837ea6d618c8e55d3b905
